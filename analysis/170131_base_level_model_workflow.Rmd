---
---
title: "Workflow of base-level model"
output: html_document
---

***
* ###Major steps


1. Use features including promoter, coding, noncoding (within-10kb of TSSs), and cg contents to adjust for mutation rate for 50-bp windows. Then apply the correcting factor of each window to every base in that window (currently the code only applies to one feature, but could be adopted to include multiple features easily).
  + 1) **[adjust_mutation_rate_window_to_base]**, this function could deal with dataset up to a few thousand genes. When applied to whole genome, will be slow and there is momory usage issue. The output of this function is a R data.table. When applied to base level, the bases that don't have mutation rate will be assigned a 0 mutation rate. These bases will be removed later in the optimization functions. Used parallel computing package in R to reduce computation time.
  + 2) **[adjust_mutation_rate_window_to_base_RCC]**, on top of what could be done from the last function, this function could deal with all genes, and could be run on RCC cluster. The output is a file with base-level information (including genename, mutation rate, and other features that might affect relative risk). It includes partition of 50-bp windows into several parts in the computation to increase efficiency. Used parallel computing package in R to reduce computation time.

2. Use mixture Poisson model to estimate the relative risk of features (could be applied to multiple features that would affect relative risk).
  + 1) **[estimate_effect_size_for_simulation_data_mixture_with_categorization]**, this function can only deal with genes that have a uniform prior probability of being risk genes.
  + 2) **[estimate_effect_size_for_simulation_data_mixture_with_categorization_v2]**, this function could handle genes with different prior probabilites. 
  + 3) **[estimate_effect_size_for_simulation_data_mixture_with_categorization_for_big_data]** (hasn't been tested on simulation data yet, but should work from the results of real whole genome data), this unction could handle genes with different prior probabilites. It could handle a big base-level mutation info file that is generated by [adjust_mutation_rate_window_to_base_RCC]. It works by reading base-level mut info file by chunks. For each chunk, do partition to extract and precompute all the parts of optimization function that don't need a predefined relative risk estimate. Then combine chunks before running optimization. 
    *   **partition_strategy**
    + 1) Partition by genename (as each gene would have different prior probabilities)
    + 2) For each gene, Partition by feature configuration. 
    + 3) Precoumpute everything that doesn't need a predefined relative risk. e.g., sum of mutation counts, sum of mutation rates, etc..., 
    + 4) in optimization function, with a defined relative risk, go through each configuation of features for each gene and sum over logP.
  + 4) **[estimate_effect_size_for_simulation_data_mixture_without_categorization]** optimization without categorization.
  
---
